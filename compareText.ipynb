{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ca5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_mat(dataset, train_rate=0.3, val_rate=0.1):\n",
    "    \"\"\"Load .mat dataset.\"\"\"\n",
    "    data = sio.loadmat(\"./data/processed/{}/{}.mat\".format(dataset, dataset))\n",
    "    text = pd.read_csv('./data/processed/{}/{}.csv'.format(dataset, dataset))\n",
    "\n",
    "\n",
    "    label = data['Label'] if ('Label' in data) else data['gnd']\n",
    "    attr = data['Attributes'] if ('Attributes' in data) else data['X']\n",
    "    network = data['Network'] if ('Network' in data) else data['A']\n",
    "    \n",
    "    adj = sp.csr_matrix(network)\n",
    "    feat = sp.lil_matrix(attr)\n",
    "\n",
    "    ano_labels = np.squeeze(np.array(label))\n",
    "\n",
    "    return adj, feat, ano_labels, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1764b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "from random import sample\n",
    "import random\n",
    "import time\n",
    "from model import CMUCL, tokenize\n",
    "from data import DataHelper\n",
    "from sklearn import preprocessing\n",
    "import dgl\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as sio\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376053f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_to_dgl_graph(adj):\n",
    "    \"\"\"Convert adjacency matrix to dgl format.\"\"\"\n",
    "    nx_graph = nx.from_scipy_sparse_array(adj)\n",
    "    dgl_graph = dgl.DGLGraph(nx_graph)\n",
    "    return dgl_graph\n",
    "\n",
    "def preprocess_features_ndarray(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(axis=1))  # sum of each row\n",
    "    r_inv = np.power(rowsum, -1).flatten()  # inverse of row sums\n",
    "    r_inv[np.isinf(r_inv)] = 0.  # replace inf with 0\n",
    "    r_mat_inv = np.diag(r_inv)  # create a diagonal matrix with r_inv\n",
    "    features = r_mat_inv.dot(features)  # row-normalize the feature matrix\n",
    "\n",
    "    return features\n",
    "\n",
    "def position_encoding(max_len, emb_size):\n",
    "    # pe = np.zeros((max_len, emb_size))\n",
    "    # position = np.arange(0, max_len)[:, np.newaxis]\n",
    "\n",
    "    pe = np.zeros((max_len, emb_size), dtype=np.float32)\n",
    "    position = np.arange(0, max_len, dtype=np.float32)[:, np.newaxis]\n",
    "\n",
    "    div_term = np.exp(np.arange(0, emb_size, 2) * -(np.log(10000.0) / emb_size))\n",
    "\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33b072a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koo/miniconda3/envs/dominant-text/lib/python3.8/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "dataset = 'Citeseer'\n",
    "adj, features, ano_label, text = load_mat(dataset) \n",
    "\n",
    "dgl_graph = adj_to_dgl_graph(adj)\n",
    "num_nodes = dgl_graph.number_of_nodes()\n",
    "\n",
    "arr_edge_index = np.vstack((dgl_graph.edges()[0].numpy(), dgl_graph.edges()[1].numpy()))\n",
    "edge_index = torch.stack(dgl_graph.edges()).to(device)\n",
    "\n",
    "node_f = features.toarray()\n",
    "\n",
    "node_f = preprocessing.StandardScaler().fit_transform(node_f)\n",
    "\n",
    "node_feat = torch.from_numpy(node_f).float()\n",
    "node_f = torch.from_numpy(node_f).to(device)\n",
    "\n",
    "tit_list = text['text'].to_numpy()\n",
    "\n",
    "start = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65cc29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, ano_label, text = load_mat(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7af8911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koo/miniconda3/envs/dominant-text/lib/python3.8/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    }
   ],
   "source": [
    "dataset = 'Citeseer'\n",
    "adj_bad, attrs_sp, label, text = load_mat(dataset)\n",
    "\n",
    "# !!! dgl_graph/edges()에서 뽑은 arr_edge_index가 유효하다는 전제 하에 사용\n",
    "dgl_graph = adj_to_dgl_graph(adj_bad)\n",
    "src = dgl_graph.edges()[0].numpy()\n",
    "dst = dgl_graph.edges()[1].numpy()\n",
    "arr_edge_index = np.vstack((src, dst))\n",
    "\n",
    "# ---- 2) arr_edge_index → A (대칭화 + 대각 0) ----\n",
    "src, dst = arr_edge_index\n",
    "num_nodes = int(max(src.max(), dst.max()) + 1)\n",
    "\n",
    "A = sp.csr_matrix(\n",
    "    (np.ones_like(src, dtype=np.float32), (src, dst)),\n",
    "    shape=(num_nodes, num_nodes)\n",
    ")\n",
    "A = A.maximum(A.T)          # 무방향이면 대칭화\n",
    "A = A.tolil()\n",
    "A.setdiag(0.0)              # ★ 대각 0으로 리셋\n",
    "A = A.tocsr()\n",
    "\n",
    "A_label = A + sp.eye(num_nodes, dtype=np.float32, format='csr')  # ★ 타깃은 A+I\n",
    "\n",
    "# ---- 3) scipy.sparse → torch.sparse ----\n",
    "def sp_to_torch_sparse(m):\n",
    "    coo = m.tocoo()\n",
    "    idx = torch.tensor(np.vstack([coo.row, coo.col]), dtype=torch.long)\n",
    "    val = torch.tensor(coo.data, dtype=torch.float32)\n",
    "    return torch.sparse_coo_tensor(idx, val, torch.Size(coo.shape))\n",
    "\n",
    "adj = sp_to_torch_sparse(A).coalesce()\n",
    "adj_label = sp_to_torch_sparse(A_label).coalesce()\n",
    "\n",
    "# ---- 4) 특징행렬 X: 정규화된 dense 텐서 ----\n",
    "# attrs_sp: (N,F) scipy.sparse\n",
    "X_np = attrs_sp.toarray()\n",
    "X_np = preprocessing.StandardScaler().fit_transform(X_np)\n",
    "X = torch.tensor(X_np, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc2d1ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Argument in Multi-Agent Systems Multi-agent systems research is concerned both with the modelling of human and animal societies and with the development of principles for the design of practical distributed information management systems. This position paper will, rather than examine the various dierences in perspective within this area of research, discuss issues of communication and commitment that are of interest to multi-agent systems research in general. 1 Introduction A computational society is a collection of autonomous agents that are loosely dependent upon each other. The intentional stance [12] is often taken in describing the state of these agents. An agent may have beliefs, desires, intentions, and it may adopt a role or have relationships with others. Thus, multi-agent systems (MAS) as with most AI research is signi cantly inuenced, at least in its vocabulary, by philosophy and cognitive psychology. 1 So, what's the point? Computational societies are developed for two primary reasons:  Mode...\",\n",
       "       'Decomposition in Data Mining: An Industrial Case Study Data mining offers tools for discovery of relationships, patterns, and knowledge in large databases. The knowledge extraction process is computationally complex and therefore a subset of all data is normally considered for mining. In this paper, numerous methods for decomposition of data sets are discussed. Decomposition enhances the quality of knowledge extracted from large databases by simplification of the data mining task. The ideas presented are illustrated with examples and an industrial case study. In the case study reported in this paper, a data mining approach is applied to extract knowledge from a data set. The extracted knowledge is used for the prediction and prevention of manufacturing faults in wafers.',\n",
       "       \"Exploration versus Exploitation in Topic Driven Crawlers Topic driven crawlers are increasingly seen as a way to address the scalability limitations of universal search engines, by distributing the crawling process across users, queries, or even client computers. The context available to a topic driven crawler allows for informed decisions about how to prioritize the links to be explored, given time and bandwidth constraints. We have developed a framework and a number of methods to evaluate the performance of topic driven crawler algorithms in a fair way, under limited memory resources. Quality metrics are derived from lexical features, link analysis, and a hybrid combination of the two. In this paper we focus on the issue of how greedy a crawler should be. Given noisy quality estimates of links in a frontier, we investigate what is an appropriate balance between a crawler's need to exploit this information to focus on the most promising links, and the need to explore links that appear suboptimal but might lead to more relevant pages. We show that exploration is essential to locate the most relevant pages under a number of quality measures, in spite of a penalty in the early stage of the crawl.\",\n",
       "       ...,\n",
       "       'Capacity-Augmenting Schema Changes on Object-Oriented Databases: Towards Increased Interoperability The realization of capacity-augmenting schema changes on a shared database while providing continued interoperability to active applications has been recognized as a hard open problem. A novel three-pronged process, called transparent object schema evolution (TOSE), is presented that successfully addresses this problem. TOSE uses the combination of views and versioning to simulate schema changes requested by one application without affecting other applications interoperating on a shared OODB. The approach is of high practical relevance as it builds upon schema evolution support offered by commercial OODBMSs. Keywords: Transparent schema evolution, object-oriented views, object-oriented databases, application migration. 1 Introduction  Current schema evolution technology suffers from the problem that schema updates on a database shared by interoperating applications often have catastrophic consequences [BKKK87, KC88, MS93, PS87, TS93, Zic91]. In such a multi-user environment, a schema c...',\n",
       "       'Agent-Based Digital Libraries: Decentralization and Coordination This paper describes agent-based systems and explains why digital libraries should be built with this type of architecture. The primary advantage of agent-based architecture is decentralization, which enables scaling, flexibility, and extensibility. The corresponding requirement is the need to coordinate agent activity. We describe the approach taken by the University of Michigan Digital Library project.  2  1 Introduction Digital libraries are just beginning to evolve. No one is certain what capabilities are needed, nor how they should be organized. It is therefore important to design digital libraries to be as open as possible, so that new collections and services can be easily added to the system. Furthermore, it is essential that libraries be able to scale to become quite large. For us, this implies a decentralized architecture, where there are few if any shared resources and where as much decision making is done as locally as possible. An example of such a distributed system is t...',\n",
       "       'Due to the large volume of such data, performance  improvements for temporal aggregation queries are critical. In this paper we examine techniques to compute  temporal aggregates that include key-range predicates (range temporal aggregates). In particular we concentrate  on SUM, COUNT and AVG aggregates.Patterns as Tools for User Interface Design . Designing usable systems is difficult and designers need effective  tools that are usable themselves. Effective design tools should be based on  proven knowledge of design. Capturing knowledge about the successful design  of usable systems is important for both novice and experienced designers and  traditionally, this knowledge has largely been described in guidelines. However,  guidelines have shown to have problems concerning selection, validity and applicability.  Patterns have emerged as a possible solution to some of the problems  from which guidelines suffer. Patterns focus on the context of a problem and solution  thereby guiding the designer in using the design knowledge. Patterns for  architecture or software engineering are not identical in structure and user interface  design also requires its own structure for patterns, focusing on usability.  This paper explores how patterns for user interface design must be structured in  order to be effective and usable tools for desig...'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tit_list = text['text'].to_numpy()\n",
    "tit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8bb917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Union, List\n",
    "from simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 128, truncate: bool = True) -> torch.LongTensor:\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the tokenized representation of given input string(s)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : Union[str, List[str]]\n",
    "        An input string or a list of input strings to tokenize\n",
    "\n",
    "    context_length : int\n",
    "        The context length to use; all CLIP models use 77 as the context length\n",
    "\n",
    "    truncate: bool\n",
    "        Whether to truncate the text in case its encoding is longer than the context length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    \n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "\n",
    "\n",
    "        \n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "928f2054",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ffca944",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token = tokenize(tit_list, context_length=context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec55bb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3186, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85afbdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koo/miniconda3/envs/dominant-text/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, torch\n",
    "\n",
    "st = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b366e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = st.encode(tit_list.tolist(),\n",
    "                    convert_to_numpy=True, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e6a4cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3186, 384)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ed28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "0. tokenizer?? \n",
    "1. 텍스트 인코더\n",
    "2. 텍스트 임베딩\n",
    "3. 기존 모델과 결합-> concat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dominant-text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
